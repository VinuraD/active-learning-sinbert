{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyAOWTDiXYvE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install baal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install baal"
      ],
      "metadata": {
        "id": "9xn5qPNQXfpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sns.set_theme()"
      ],
      "metadata": {
        "id": "0PS3gDD3XhdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the seed: built-in python, numpy, and pytorch\n",
        "seed = 13\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "id": "snE-eq6iXjdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "0bpX20C7XlNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel,AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "7aqiRpi7XnWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nbroad/ESG-BERT\")\n",
        "model=AutoModelForSequenceClassification.from_pretrained(\"nbroad/ESG-BERT\",num_labels=2,ignore_mismatched_sizes=True)"
      ],
      "metadata": {
        "id": "JTRaNYI6XpSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "with open('/content/climate-fever-dataset-r1.jsonl') as f:\n",
        "    for jsonObj in f:\n",
        "        t = json.loads(jsonObj)\n",
        "        text.append(t)"
      ],
      "metadata": {
        "id": "kujTscE_Xttf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(columns=['claim','label'])\n",
        "for i in text:\n",
        "  row={'claim':i['claim'],'label':i['claim_label']}\n",
        "  df=df.append(row,ignore_index=True)\n",
        "\n",
        "df['label']=df['label'].apply(lambda x: 1 if x=='SUPPORTS' else 0)"
      ],
      "metadata": {
        "id": "64N9nRLsXv18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_index=random.sample(range(0,1534),1380)\n",
        "labeled_index=list(set(range(1534))-set(unlabeled_index))\n"
      ],
      "metadata": {
        "id": "N7cei1ZiXyPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized=tokenizer(all_data['claim'].to_list(), truncation=True,max_length=330,padding=True,return_tensors='pt')"
      ],
      "metadata": {
        "id": "uwXkfV5mX0W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        #item = {'text':self.text,'label':self.labels}\n",
        "        item['label'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "VQiIyIr-X2Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#raw_train_set=Dataset(all_data['claim'].to_list(),all_data['label'].to_list())\n",
        "raw_train_set=Dataset(train_tokenized,all_data['label'].to_list())"
      ],
      "metadata": {
        "id": "opuxvWIZX4jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "active_set = ActiveLearningDataset(raw_train_set)\n",
        "active_set.can_label = False # Need to manually do this for research\n",
        "\n",
        "# lets randomly label 100 samples, therefore len(active_set) should be 100\n",
        "active_set.label_randomly(150)\n",
        "assert len(active_set) == 150\n",
        "print(len(active_set.pool))"
      ],
      "metadata": {
        "id": "8icjYjpqX6d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "active_set.n_labelled"
      ],
      "metadata": {
        "id": "unuNRIKeX9aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "active_set.n_unlabelled\n"
      ],
      "metadata": {
        "id": "hXPzTMFzX_P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "model = patch_module(model)\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "init_weights = deepcopy(model.state_dict())"
      ],
      "metadata": {
        "id": "6rZAfcUcYBEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from baal.active import get_heuristic\n",
        "\n",
        "heuristic = get_heuristic('bald')"
      ],
      "metadata": {
        "id": "f6-jbr5nYC91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from baal.transformers_trainer_wrapper import BaalTransformersTrainer\n",
        "from baal.active.active_loop import ActiveLearningLoop"
      ],
      "metadata": {
        "id": "UWRudVNtYEwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='outputs',  # output directory\n",
        "    num_train_epochs=4,  # total # of training epochs per AL step\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,  # batch size for evaluation\n",
        "    #weight_decay=0.01,  # strength of weight decay\n",
        "    logging_dir='logs',  # directory for storing logs\n",
        "    learning_rate=1e-4, #0.00012922855,\n",
        "    overwrite_output_dir=True,\n",
        "    )\n",
        "\n",
        "# create the trainer through Baal Wrapper\n",
        "baal_trainer = BaalTransformersTrainer(model=model,\n",
        "                                       args=training_args,\n",
        "                                       train_dataset=active_set,\n",
        "                                       tokenizer=None)\n",
        "\n",
        "\n",
        "active_loop = ActiveLearningLoop(active_set,\n",
        "                                 baal_trainer.predict_on_dataset,\n",
        "                                 heuristic, 10, iterations=3)\n",
        "\n",
        "for epoch in range(2):\n",
        "    baal_trainer.train()\n",
        "    \n",
        "    should_continue = active_loop.step()\n",
        "\n",
        "    # We reset the model weights to relearn from the new train set.\n",
        "    baal_trainer.load_state_dict(init_weights)\n",
        "    baal_trainer.lr_scheduler = None\n",
        "    if not should_continue:\n",
        "        break\n",
        "\n",
        "# at each Active step we add 10 samples to labelled data. At this point we should have 30 samples added\n",
        "# to the labelled part of training set.\n",
        "print(len(active_set))"
      ],
      "metadata": {
        "id": "5eLkG-aCYGUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}